xception_preprocess_input()
dim(img_array) <- c(1, dim(img_array))
img_array
}
classification_generator <-
function(data,
target_height,
target_width,
shuffle,
batch_size) {
i <- 1
function() {
if (shuffle) {
indices <- sample(1:nrow(data), size = batch_size)
} else {
if (i + batch_size >= nrow(data))
i <<- 1
indices <- c(i:min(i + batch_size - 1, nrow(data)))
i <<- i + length(indices)
}
x <-
array(0, dim = c(length(indices), target_height, target_width, 3))
y <- array(0, dim = c(length(indices), 1))
for (j in 1:length(indices)) {
x[j, , , ] <-
load_and_preprocess_image(data[[indices[j], "file_name"]],
target_height, target_width)
y[j, ] <-
data[[indices[j], "category_id"]] - 1
}
x <- x / 255
list(x, y)
}
}
train_gen <- classification_generator(
train_data,
target_height = target_height,
target_width = target_width,
shuffle = TRUE,
batch_size = batch_size
)
valid_gen <- classification_generator(
validation_data,
target_height = target_height,
target_width = target_width,
shuffle = FALSE,
batch_size = batch_size
)
model %>% fit_generator(
train_gen,
epochs = 20,
steps_per_epoch = nrow(train_data) / batch_size,
validation_data = valid_gen,
validation_steps = nrow(validation_data) / batch_size,
callbacks = list(
callback_model_checkpoint(
file.path("class_only", "weights.{epoch:02d}-{val_loss:.2f}.hdf5")
),
callback_early_stopping(patience = 2)
)
)
source('~/GitHub/NIAB_Rotation/Fruit_model/Run/params.R')
source('~/GitHub/NIAB_Rotation/Fruit_model/Data/Data_Functions.R') # not in parallel # contains functions, 'labeller', 'Data_in_final_form'
source('~/GitHub/NIAB_Rotation/Fruit_model/Data/Cluster_data_functions.R') # in parallel
source('~/GitHub/NIAB_Rotation/Fruit_model/Analysis/Functions.R') # contains functions 'image_tester', 'preds', 'multipreds', 'image_predictor'
source('~/GitHub/NIAB_Rotation/Fruit_model/Model/Fruit_model.R')  # needs params   # contains function 'create_fruit_model' # needs to be before data_producer, so library(keras) is before library(reticulate)?
source('~/GitHub/NIAB_Rotation/Fruit_model/Data/Data_Producer.R') # slow to run, saves train & test data & labels
library(keras)
library(dplyr)
library(ggplot2)
library(tidyr)
# data comes in from Data_Producer
Train_data_saved   <- readRDS(params$train_data_name)
Train_labels_saved <- readRDS(params$train_label_name)
Test_data_saved    <- readRDS(params$test_data_name)
Test_labels_saved  <- readRDS(params$test_label_name)
############################################
# fit model
model_name <- "fruit_model_new.h5" # "fruit_model.h5"
model_my_own <-create_fruit_model(Train_data_saved[,,,params$channel_no,drop = F],Train_labels_saved,Test_data_saved[,,,params$channel_no,drop = F],Test_labels_saved) # drop = F stops R collapsing array to 3 dimensions not 4
model_my_own %>% summary()
model_my_own %>% save_model_hdf5(model_name)
# below model has all fruits
#new_model <- load_model_hdf5("fruit_model.h5")
#new_model %>% summary()
results <- model_my_own %>% evaluate(Test_data_saved[,,,params$channel_no,drop=F], Test_labels_saved)
results
results <- model_my_own %>% predict_classes(Test_data_saved[,,,params$channel_no,drop = F]) # drop = F stops R collapsing array to 3 dimensions not 4
############################################
data <- image_tester(params$internet_path,"Braeburn")
res2 <- model_my_own %>% predict(data)
preds(res2)
multipreds(res2,params$number_probs)
params <- list('img_dir' = "C:/Users/Administrator/Documents/GitHub/test_images_to_use/all2",
'annot_file' = "C:/Users/Administrator/Documents/GitHub/test_images_to_use/jsonfold/online.json",
'folder_containing_scripts' = "C:/Users/Administrator/Documents/GitHub/NIAB_Rotation/Fruit_model/Pipeline",
'folder_to_save_model_in' = "C:/Users/Administrator/Documents/GitHub",
'folder_to_save_images_in' = "C:/Users/Administrator/Documents/GitHub/Pipeline_resulting_images",
'target_height' = 224,
'target_width' = 224,
'batch_size' = 1, #10 #1 # low is faster but less accurate?   ### 4
'proportion_of_samples' = 0.1, # 0.2, but was classifying everything the same. ###0.3
'threshold' = 0.4,
'class_background' = length(class_list), # should it be length(class_list), or length(class_list) + 1?
'cl_output' = length(class_list), # 20
'epochs' = 20,
'weight_file_path' = "C:/Users/Administrator/Documents/GitHub/Weights",
'label_names' = class_list,
'layer_units' = 16, #256, # 30
'patience' = 2, # was 8, but that's quite slow
'save' = 1, #save model?
'model_name' = "disease_image_classifier.h5"
)
class_list <- c("YR","MSD","BS") # from json
params <- list('img_dir' = "C:/Users/Administrator/Documents/GitHub/test_images_to_use/all2",
'annot_file' = "C:/Users/Administrator/Documents/GitHub/test_images_to_use/jsonfold/online.json",
'folder_containing_scripts' = "C:/Users/Administrator/Documents/GitHub/NIAB_Rotation/Fruit_model/Pipeline",
'folder_to_save_model_in' = "C:/Users/Administrator/Documents/GitHub",
'folder_to_save_images_in' = "C:/Users/Administrator/Documents/GitHub/Pipeline_resulting_images",
'target_height' = 224,
'target_width' = 224,
'batch_size' = 1, #10 #1 # low is faster but less accurate?   ### 4
'proportion_of_samples' = 0.1, # 0.2, but was classifying everything the same. ###0.3
'threshold' = 0.4,
'class_background' = length(class_list), # should it be length(class_list), or length(class_list) + 1?
'cl_output' = length(class_list), # 20
'epochs' = 20,
'weight_file_path' = "C:/Users/Administrator/Documents/GitHub/Weights",
'label_names' = class_list,
'layer_units' = 16, #256, # 30
'patience' = 2, # was 8, but that's quite slow
'save' = 1, #save model?
'model_name' = "disease_image_classifier.h5"
)
CNN_model
feature_extractor
feature_extractor$input
if(feature_extractor$input=NULL){
feature_extractor <- application_xception(
include_top = FALSE,
input_shape = c(224, 224, 3)
)
}
is.null(feature_extractor$input)
if(is.null(feature_extractor$input)){
feature_extractor <- application_xception(
include_top = FALSE,
input_shape = c(224, 224, 3)
)
}
# setwd(params$folder_containing_scripts)
# source('parameters.R')
# source('Image_classifier_functions.R')
# how many of these necessary?
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(XML)
library(xml2)
library(jsonlite)
library(tensorflow)
######################################################################
# added from image_classifier_functions
create_image_container <- function(annotation){
imageinfo <- annotation$images %>% {
tibble(
id = map_dbl(.$id, c),
file_name = map_chr(.$file_name, c),
image_height = map_dbl(.$height, c),
image_width = map_dbl(.$width, c)
)
}
# Load bounding box
boxinfo <- annotation$annotations %>% {
tibble(
image_id = map_dbl(.$image_id, c),
category_id = map_dbl(.$category_id, c),
bbox = map(.$bbox, c)
)
}
boxinfo <- boxinfo %>%
mutate(bbox = unlist(map(.$bbox, function(x) paste(x, collapse = " "))))
boxinfo <- boxinfo %>%
separate(bbox, into = c("x_left", "y_top", "bbox_width", "bbox_height"))
boxinfo <- boxinfo %>% mutate_all(as.numeric)
#As usual in image processing, the y axis starts from the top.
boxinfo <- boxinfo %>%
mutate(y_bottom = y_top + bbox_height - 1, x_right = x_left + bbox_width - 1)
#Finally, we still need to match class ids to class names.
catinfo <- annotation$categories %>%  {
tibble(id = map_dbl(.$id, c), name = map_chr(.$name, c))
}
#So, putting it all together:
imageinfo <- imageinfo %>%
inner_join(boxinfo, by = c("id" = "image_id")) %>%
inner_join(catinfo, by = c("category_id" = "id"))
return(imageinfo)
}
scale_image_boundingbox <- function(imageinfo, target_height, target_width){
imageinfo <- imageinfo %>% mutate(
x_left_scaled = (x_left / image_width * target_width) %>% round(),
x_right_scaled = (x_right / image_width * target_width) %>% round(),
y_top_scaled = (y_top / image_height * target_height) %>% round(),
y_bottom_scaled = (y_bottom / image_height * target_height) %>% round(),
bbox_width_scaled =  (bbox_width / image_width * target_width) %>% round(),
bbox_height_scaled = (bbox_height / image_height * target_height) %>% round()
)
return(imageinfo)
}
######################################################################
annotations <- jsonlite::fromJSON(txt = params$annot_file)
# create image object
imageinfo <- create_image_container(annotations)
# Scale bbox
imageinfo <- scale_image_boundingbox(imageinfo, params$target_height, params$target_width)
n_samples <- nrow(imageinfo)
set.seed(params$seed) # seed
train_indices <- sample(1:n_samples, params$proportion_of_samples * n_samples)
train_data <- imageinfo[train_indices,]
validation_data <- imageinfo[-train_indices,]
# Data generator
image_size <- params$target_width # same as height
# is slow so only run if necessary
if(is.null(feature_extractor$input)){
feature_extractor <- application_xception(
include_top = FALSE,
input_shape = c(224, 224, 3)
)
}
is.null(feature_extractor$input
)
source('C:/Users/Administrator/Documents/GitHub/NIAB_Rotation/Fruit_model/Pipeline/parameters.R')
setwd(params$folder_containing_scripts)
### load libraries
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(XML)
library(xml2)
library(jsonlite)
library(tensorflow)
# list existing models, informs choice of one to load if we are loading
Model_names<-as.data.frame(list.files(path = params$folder_to_save_model_in, pattern=".h5", all.files=T, full.names=F, no.. = T))
colnames(Model_names)<-'Model_Names'
Model_names
##################################################################################
# changable things in this section
run_xml_to_json <- 0    # use new data?
run_model_trainer <- 1 # train model, or just load an existing one?
# if train, then save. If not, then load.
## pick which model to load if loading # 1, 2, or 3?
params$model_name_to_load <- as.character(Model_names[1,]) # 1, 2, or 3?
# vector with one or more components to train over.
proportion_samples_vec_input<- c(0.6) #seq(0.1,0.7,0.3)
epochs_vec_input            <- c(2) #25  #seq(10,40,15)
batch_size_vec_input        <- c(5) #seq(1,7,2) # c(5)
layers_vec_input            <- c(20)#seq(260,280,20) #c(40,80,128) #c(160,256,320,448,512) #28,192,256)
##################################################################################
params$save <- run_model_trainer     # save model? most of the time this should agree with run_model_trainer, but sometimes we might want to not save a model that we just trained
# if CNN_model is already in the environment, can change to params$load <- 0 to save computational time. If we are training a new model, don't load. Otherwise load another.
params$load <- 1 - run_model_trainer
# don't change anything from here down. All settings should be accessible from above.
##################################################################################
# adds our annotations to the relevant images
if(run_xml_to_json==1){
# source('save_to_xml.R',echo= TRUE) # with semi-automated json, not using
system("python xmltojson.py")
}
##################################################################################
# CNN
source('CNN_data_generator_and_model_functions.R',echo= TRUE) # generates data and contains functions called upon in CNN model, SVM model and the output analysis file.
# check that approx even number of each category present
for(k in 1:length(params$label_names)){
print(paste("Number of training images in category",params$label_names[k],"is",sum(train_data$category_id==k)))
}
if(run_model_trainer==1){
grid_output<-grid(proportion_samples_vec_input,epochs_vec_input,batch_size_vec_input,layers_vec_input)
model<-grid_output$best_model # use model with best val_class_acc
print(grid_output$grid_results)
params$proportion_of_samples <- grid_output$best_params$Proportion_Samples
params$epochs <- grid_output$best_params$Epochs
params$batch_size <- grid_output$best_params$Batch_Size
params$layer_units <- grid_output$best_params$Layers
setwd(params$folder_to_save_data_in)
saveRDS(as.data.frame(grid_output$grid_results),file=paste("grid_output",paste(layers_vec_input,collapse="_"),sep="_"))
setwd(params$folder_containing_scripts)
}
if(run_model_trainer==1){
grid_output<-grid(proportion_samples_vec_input,epochs_vec_input,batch_size_vec_input,layers_vec_input)
model<-grid_output$best_model # use model with best val_class_acc
print(grid_output$grid_results)
params$proportion_of_samples <- grid_output$best_params$Proportion_Samples
params$epochs <- grid_output$best_params$Epochs
params$batch_size <- grid_output$best_params$Batch_Size
params$layer_units <- grid_output$best_params$Layers
setwd(params$folder_to_save_data_in)
saveRDS(as.data.frame(grid_output$grid_results),file=paste("grid_output",paste(layers_vec_input,collapse="_"),sep="_"))
setwd(params$folder_containing_scripts)
}
Data_file_names<-as.data.frame(list.files(path = params$folder_to_save_data_in, pattern="grid_output", all.files=T, full.names=F, no.. = T))
colnames(Data_file_names)<-'Data_file_names'
##################################################################################
if(params$save == 1){
x<- paste0("Disease_CNN_proportion-samples-",params$proportion_of_samples,"-epochs-",params$epochs,"-batch_size-",params$batch_size,"-layers-",params$layer_units)
x1<-gsub("\\.","_",x)
params$model_name_to_save<-paste0(x1,".h5") # name now more descriptive
setwd(params$folder_to_save_model_in)
model %>% save_model_hdf5(params$model_name_to_save)
setwd(params$folder_containing_scripts)
}
##################################################################################
source('CNN_output_analysis.R',echo= TRUE) # analyse resulting CNN (or a loaded CNN)
table_train  # confusion matrix
table_val # confusi
source('C:/Users/Administrator/Documents/GitHub/NIAB_Rotation/Fruit_model/Pipeline/parameters.R')
setwd(params$folder_containing_scripts)
### load libraries
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(XML)
library(xml2)
library(jsonlite)
library(tensorflow)
# list existing models, informs choice of one to load if we are loading
Model_names<-as.data.frame(list.files(path = params$folder_to_save_model_in, pattern=".h5", all.files=T, full.names=F, no.. = T))
colnames(Model_names)<-'Model_Names'
Model_names
##################################################################################
# changable things in this section
run_xml_to_json <- 0    # use new data?
run_model_trainer <- 1 # train model, or just load an existing one?
# if train, then save. If not, then load.
## pick which model to load if loading # 1, 2, or 3?
params$model_name_to_load <- as.character(Model_names[1,]) # 1, 2, or 3?
# vector with one or more components to train over.
proportion_samples_vec_input<- c(0.6) #seq(0.1,0.7,0.3)
epochs_vec_input            <- c(2) #25  #seq(10,40,15)
batch_size_vec_input        <- c(5) #seq(1,7,2) # c(5)
layers_vec_input            <- c(20)#seq(260,280,20) #c(40,80,128) #c(160,256,320,448,512) #28,192,256)
##################################################################################
params$save <- run_model_trainer     # save model? most of the time this should agree with run_model_trainer, but sometimes we might want to not save a model that we just trained
# if CNN_model is already in the environment, can change to params$load <- 0 to save computational time. If we are training a new model, don't load. Otherwise load another.
params$load <- 1 - run_model_trainer
# don't change anything from here down. All settings should be accessible from above.
##################################################################################
# adds our annotations to the relevant images
if(run_xml_to_json==1){
# source('save_to_xml.R',echo= TRUE) # with semi-automated json, not using
system("python xmltojson.py")
}
##################################################################################
# CNN
source('CNN_data_generator_and_model_functions.R',echo= TRUE) # generates data and contains functions called upon in CNN model, SVM model and the output analysis file.
# check that approx even number of each category present
for(k in 1:length(params$label_names)){
print(paste("Number of training images in category",params$label_names[k],"is",sum(train_data$category_id==k)))
}
if(run_model_trainer==1){
grid_output<-grid(proportion_samples_vec_input,epochs_vec_input,batch_size_vec_input,layers_vec_input)
model<-grid_output$best_model # use model with best val_class_acc
print(grid_output$grid_results)
params$proportion_of_samples <- grid_output$best_params$Proportion_Samples
params$epochs <- grid_output$best_params$Epochs
params$batch_size <- grid_output$best_params$Batch_Size
params$layer_units <- grid_output$best_params$Layers
setwd(params$folder_to_save_data_in)
saveRDS(as.data.frame(grid_output$grid_results),file=paste("grid_output",paste(layers_vec_input,collapse="_"),sep="_"))
setwd(params$folder_containing_scripts)
}
Data_file_names<-as.data.frame(list.files(path = params$folder_to_save_data_in, pattern="grid_output", all.files=T, full.names=F, no.. = T))
colnames(Data_file_names)<-'Data_file_names'
##################################################################################
if(params$save == 1){
x<- paste0("Disease_CNN_proportion-samples-",params$proportion_of_samples,"-epochs-",params$epochs,"-batch_size-",params$batch_size,"-layers-",params$layer_units)
x1<-gsub("\\.","_",x)
params$model_name_to_save<-paste0(x1,".h5") # name now more descriptive
setwd(params$folder_to_save_model_in)
model %>% save_model_hdf5(params$model_name_to_save)
setwd(params$folder_containing_scripts)
}
##################################################################################
source('CNN_output_analysis.R',echo= TRUE) # analyse resulting CNN (or a loaded CNN)
table_train  # confusion matrix
table_val # confusion matrix
source('C:/Users/Administrator/Documents/GitHub/NIAB_Rotation/Fruit_model/Pipeline/parameters.R')
setwd(params$folder_containing_scripts)
### load libraries
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(XML)
library(xml2)
library(jsonlite)
library(tensorflow)
# list existing models, informs choice of one to load if we are loading
Model_names<-as.data.frame(list.files(path = params$folder_to_save_model_in, pattern=".h5", all.files=T, full.names=F, no.. = T))
colnames(Model_names)<-'Model_Names'
Model_names
##################################################################################
# changable things in this section
run_xml_to_json <- 0    # use new data?
run_model_trainer <- 1 # train model, or just load an existing one?
# if train, then save. If not, then load.
## pick which model to load if loading # 1, 2, or 3?
params$model_name_to_load <- as.character(Model_names[1,]) # 1, 2, or 3?
# vector with one or more components to train over.
proportion_samples_vec_input<- c(0.6) #seq(0.1,0.7,0.3)
epochs_vec_input            <- c(2) #25  #seq(10,40,15)
batch_size_vec_input        <- c(5) #seq(1,7,2) # c(5)
layers_vec_input            <- c(20)#seq(260,280,20) #c(40,80,128) #c(160,256,320,448,512) #28,192,256)
##################################################################################
params$save <- run_model_trainer     # save model? most of the time this should agree with run_model_trainer, but sometimes we might want to not save a model that we just trained
# if CNN_model is already in the environment, can change to params$load <- 0 to save computational time. If we are training a new model, don't load. Otherwise load another.
params$load <- 1 - run_model_trainer
# don't change anything from here down. All settings should be accessible from above.
##################################################################################
# adds our annotations to the relevant images
if(run_xml_to_json==1){
# source('save_to_xml.R',echo= TRUE) # with semi-automated json, not using
system("python xmltojson.py")
}
##################################################################################
# CNN
source('CNN_data_generator_and_model_functions.R',echo= TRUE) # generates data and contains functions called upon in CNN model, SVM model and the output analysis file.
# check that approx even number of each category present
for(k in 1:length(params$label_names)){
print(paste("Number of training images in category",params$label_names[k],"is",sum(train_data$category_id==k)))
}
if(run_model_trainer==1){
grid_output<-grid(proportion_samples_vec_input,epochs_vec_input,batch_size_vec_input,layers_vec_input)
model<-grid_output$best_model # use model with best val_class_acc
print(grid_output$grid_results)
params$proportion_of_samples <- grid_output$best_params$Proportion_Samples
params$epochs <- grid_output$best_params$Epochs
params$batch_size <- grid_output$best_params$Batch_Size
params$layer_units <- grid_output$best_params$Layers
setwd(params$folder_to_save_data_in)
saveRDS(as.data.frame(grid_output$grid_results),file=paste("grid_output",paste(layers_vec_input,collapse="_"),sep="_"))
setwd(params$folder_containing_scripts)
}
Data_file_names<-as.data.frame(list.files(path = params$folder_to_save_data_in, pattern="grid_output", all.files=T, full.names=F, no.. = T))
colnames(Data_file_names)<-'Data_file_names'
if(run_model_trainer==1){
grid_output<-grid(proportion_samples_vec_input,epochs_vec_input,batch_size_vec_input,layers_vec_input)
model<-grid_output$best_model # use model with best val_class_acc
print(grid_output$grid_results)
params$proportion_of_samples <- grid_output$best_params$Proportion_Samples
params$epochs <- grid_output$best_params$Epochs
params$batch_size <- grid_output$best_params$Batch_Size
params$layer_units <- grid_output$best_params$Layers
setwd(params$folder_to_save_data_in)
saveRDS(as.data.frame(grid_output$grid_results),file=paste("grid_output",paste(layers_vec_input,collapse="_"),sep="_"))
setwd(params$folder_containing_scripts)
}
Data_file_names<-as.data.frame(list.files(path = params$folder_to_save_data_in, pattern="grid_output", all.files=T, full.names=F, no.. = T))
colnames(Data_file_names)<-'Data_file_names'
arrange(tr_data,file_name)
arrange(train_data,file_name)
grid_output<-grid(proportion_samples_vec_input,epochs_vec_input,batch_size_vec_input,layers_vec_input)
if(run_model_trainer==1){
grid_output<-grid(proportion_samples_vec_input,epochs_vec_input,batch_size_vec_input,layers_vec_input)
model<-grid_output$best_model # use model with best val_class_acc
print(grid_output$grid_results)
params$proportion_of_samples <- grid_output$best_params$Proportion_Samples
params$epochs <- grid_output$best_params$Epochs
params$batch_size <- grid_output$best_params$Batch_Size
params$layer_units <- grid_output$best_params$Layers
setwd(params$folder_to_save_data_in)
saveRDS(as.data.frame(grid_output$grid_results),file=paste("grid_output",paste(layers_vec_input,collapse="_"),sep="_"))
setwd(params$folder_containing_scripts)
}
Data_file_names<-as.data.frame(list.files(path = params$folder_to_save_data_in, pattern="grid_output", all.files=T, full.names=F, no.. = T))
colnames(Data_file_names)<-'Data_file_names'
##################################################################################
if(params$save == 1){
x<- paste0("Disease_CNN_proportion-samples-",params$proportion_of_samples,"-epochs-",params$epochs,"-batch_size-",params$batch_size,"-layers-",params$layer_units)
x1<-gsub("\\.","_",x)
params$model_name_to_save<-paste0(x1,".h5") # name now more descriptive
setwd(params$folder_to_save_model_in)
model %>% save_model_hdf5(params$model_name_to_save)
setwd(params$folder_containing_scripts)
}
##################################################################################
source('CNN_output_analysis.R',echo= TRUE) # analyse resulting CNN (or a loaded CNN)
table_train  # confusion matrix
table_val # confusion matrix
grid_output$grid_results

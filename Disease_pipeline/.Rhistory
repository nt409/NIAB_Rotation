word_index <- dataset_imdb_word_index()
paste0("Training entries: ", length(train_data), ", labels: ", length(train_labels))
# integer form for review 1
#train_data[[1]]
# reviews should be same length - resolved later
#length(train_data[[1]])
#length(train_data[[2]])
word_index_df <- data.frame(
word = names(word_index),
idx = unlist(word_index, use.names = FALSE),
stringsAsFactors = FALSE
)
# The first indices are reserved
word_index_df <- word_index_df %>% mutate(idx = idx + 3)
word_index_df <- word_index_df %>%
add_row(word = "<PAD>", idx = 0)%>%
add_row(word = "<START>", idx = 1)%>%
add_row(word = "<UNK>", idx = 2)%>%
add_row(word = "<UNUSED>", idx = 3)
word_index_df <- word_index_df %>% arrange(idx)
decode_review <- function(text){
paste(map(text, function(number) word_index_df %>%
filter(idx == number) %>%
select(word) %>%
pull()),
collapse = " ")
}
decode_review(train_data[[2]])
# padding at the end
train_data <- pad_sequences(
train_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
test_data <- pad_sequences(
test_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
# pick 10000 values from original training data
x_val <- train_data[1:10000, ]
partial_x_train <- train_data[10001:nrow(train_data), ]
y_val <- train_labels[1:10000]
partial_y_train <- train_labels[10001:length(train_labels)]
newList <- list("x_val" = x_val,"partial_x_train" = partial_x_train,"y_val" = y_val,"partial_y_train" = partial_y_train,"test_data"=test_data,"test_labels"=test_labels)
#return(newList)
#}
#Movie_data <- function(){
imdb <- dataset_imdb(num_words = 10000)
c(train_data, train_labels) %<-% imdb$train
c(test_data, test_labels) %<-% imdb$test
#num words discards rare words and keeps 10000 most used words
# maps words to integers
word_index <- dataset_imdb_word_index()
paste0("Training entries: ", length(train_data), ", labels: ", length(train_labels))
# integer form for review 1
#train_data[[1]]
# reviews should be same length - resolved later
#length(train_data[[1]])
#length(train_data[[2]])
word_index_df <- data.frame(
word = names(word_index),
idx = unlist(word_index, use.names = FALSE),
stringsAsFactors = FALSE
)
# The first indices are reserved
word_index_df <- word_index_df %>% mutate(idx = idx + 3)
word_index_df <- word_index_df %>%
add_row(word = "<PAD>", idx = 0)%>%
add_row(word = "<START>", idx = 1)%>%
add_row(word = "<UNK>", idx = 2)%>%
add_row(word = "<UNUSED>", idx = 3)
word_index_df <- word_index_df %>% arrange(idx)
decode_review <- function(text){
paste(map(text, function(number) word_index_df %>%
filter(idx == number) %>%
select(word) %>%
pull()),
collapse = " ")
}
decode_review(train_data[[3]])
# padding at the end
train_data <- pad_sequences(
train_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
test_data <- pad_sequences(
test_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
# pick 10000 values from original training data
x_val <- train_data[1:10000, ]
partial_x_train <- train_data[10001:nrow(train_data), ]
y_val <- train_labels[1:10000]
partial_y_train <- train_labels[10001:length(train_labels)]
newList <- list("x_val" = x_val,"partial_x_train" = partial_x_train,"y_val" = y_val,"partial_y_train" = partial_y_train,"test_data"=test_data,"test_labels"=test_labels)
#return(newList)
#}
#Movie_data <- function(){
imdb <- dataset_imdb(num_words = 10000)
c(train_data, train_labels) %<-% imdb$train
c(test_data, test_labels) %<-% imdb$test
#num words discards rare words and keeps 10000 most used words
# maps words to integers
word_index <- dataset_imdb_word_index()
paste0("Training entries: ", length(train_data), ", labels: ", length(train_labels))
# integer form for review 1
#train_data[[1]]
# reviews should be same length - resolved later
#length(train_data[[1]])
#length(train_data[[2]])
word_index_df <- data.frame(
word = names(word_index),
idx = unlist(word_index, use.names = FALSE),
stringsAsFactors = FALSE
)
# The first indices are reserved
word_index_df <- word_index_df %>% mutate(idx = idx + 3)
word_index_df <- word_index_df %>%
add_row(word = "<PAD>", idx = 0)%>%
add_row(word = "<START>", idx = 1)%>%
add_row(word = "<UNK>", idx = 2)%>%
add_row(word = "<UNUSED>", idx = 3)
word_index_df <- word_index_df %>% arrange(idx)
decode_review <- function(text){
paste(map(text, function(number) word_index_df %>%
filter(idx == number) %>%
select(word) %>%
pull()),
collapse = " ")
}
decode_review(train_data[[4]])
# padding at the end
train_data <- pad_sequences(
train_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
test_data <- pad_sequences(
test_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
# pick 10000 values from original training data
x_val <- train_data[1:10000, ]
partial_x_train <- train_data[10001:nrow(train_data), ]
y_val <- train_labels[1:10000]
partial_y_train <- train_labels[10001:length(train_labels)]
newList <- list("x_val" = x_val,"partial_x_train" = partial_x_train,"y_val" = y_val,"partial_y_train" = partial_y_train,"test_data"=test_data,"test_labels"=test_labels)
#return(newList)
#}
#Movie_data <- function(){
imdb <- dataset_imdb(num_words = 10000)
c(train_data, train_labels) %<-% imdb$train
c(test_data, test_labels) %<-% imdb$test
#num words discards rare words and keeps 10000 most used words
# maps words to integers
word_index <- dataset_imdb_word_index()
paste0("Training entries: ", length(train_data), ", labels: ", length(train_labels))
# integer form for review 1
#train_data[[1]]
# reviews should be same length - resolved later
#length(train_data[[1]])
#length(train_data[[2]])
word_index_df <- data.frame(
word = names(word_index),
idx = unlist(word_index, use.names = FALSE),
stringsAsFactors = FALSE
)
# The first indices are reserved
word_index_df <- word_index_df %>% mutate(idx = idx + 3)
word_index_df <- word_index_df %>%
add_row(word = "<PAD>", idx = 0)%>%
add_row(word = "<START>", idx = 1)%>%
add_row(word = "<UNK>", idx = 2)%>%
add_row(word = "<UNUSED>", idx = 3)
word_index_df <- word_index_df %>% arrange(idx)
decode_review <- function(text){
paste(map(text, function(number) word_index_df %>%
filter(idx == number) %>%
select(word) %>%
pull()),
collapse = " ")
}
decode_review(train_data[[5]])
# padding at the end
train_data <- pad_sequences(
train_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
test_data <- pad_sequences(
test_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
# pick 10000 values from original training data
x_val <- train_data[1:10000, ]
partial_x_train <- train_data[10001:nrow(train_data), ]
y_val <- train_labels[1:10000]
partial_y_train <- train_labels[10001:length(train_labels)]
newList <- list("x_val" = x_val,"partial_x_train" = partial_x_train,"y_val" = y_val,"partial_y_train" = partial_y_train,"test_data"=test_data,"test_labels"=test_labels)
#return(newList)
#}
#Movie_data <- function(){
imdb <- dataset_imdb(num_words = 10000)
c(train_data, train_labels) %<-% imdb$train
c(test_data, test_labels) %<-% imdb$test
#num words discards rare words and keeps 10000 most used words
# maps words to integers
word_index <- dataset_imdb_word_index()
paste0("Training entries: ", length(train_data), ", labels: ", length(train_labels))
# integer form for review 1
#train_data[[1]]
# reviews should be same length - resolved later
#length(train_data[[1]])
#length(train_data[[2]])
word_index_df <- data.frame(
word = names(word_index),
idx = unlist(word_index, use.names = FALSE),
stringsAsFactors = FALSE
)
# The first indices are reserved
word_index_df <- word_index_df %>% mutate(idx = idx + 3)
word_index_df <- word_index_df %>%
add_row(word = "<PAD>", idx = 0)%>%
add_row(word = "<START>", idx = 1)%>%
add_row(word = "<UNK>", idx = 2)%>%
add_row(word = "<UNUSED>", idx = 3)
word_index_df <- word_index_df %>% arrange(idx)
decode_review <- function(text){
paste(map(text, function(number) word_index_df %>%
filter(idx == number) %>%
select(word) %>%
pull()),
collapse = " ")
}
decode_review(train_data[[6]])
# padding at the end
train_data <- pad_sequences(
train_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
test_data <- pad_sequences(
test_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
# pick 10000 values from original training data
x_val <- train_data[1:10000, ]
partial_x_train <- train_data[10001:nrow(train_data), ]
y_val <- train_labels[1:10000]
partial_y_train <- train_labels[10001:length(train_labels)]
newList <- list("x_val" = x_val,"partial_x_train" = partial_x_train,"y_val" = y_val,"partial_y_train" = partial_y_train,"test_data"=test_data,"test_labels"=test_labels)
#return(newList)
#}
#Movie_data <- function(){
imdb <- dataset_imdb(num_words = 10000)
c(train_data, train_labels) %<-% imdb$train
c(test_data, test_labels) %<-% imdb$test
#num words discards rare words and keeps 10000 most used words
# maps words to integers
word_index <- dataset_imdb_word_index()
paste0("Training entries: ", length(train_data), ", labels: ", length(train_labels))
# integer form for review 1
#train_data[[1]]
# reviews should be same length - resolved later
#length(train_data[[1]])
#length(train_data[[2]])
word_index_df <- data.frame(
word = names(word_index),
idx = unlist(word_index, use.names = FALSE),
stringsAsFactors = FALSE
)
# The first indices are reserved
word_index_df <- word_index_df %>% mutate(idx = idx + 3)
word_index_df <- word_index_df %>%
add_row(word = "<PAD>", idx = 0)%>%
add_row(word = "<START>", idx = 1)%>%
add_row(word = "<UNK>", idx = 2)%>%
add_row(word = "<UNUSED>", idx = 3)
word_index_df <- word_index_df %>% arrange(idx)
decode_review <- function(text){
paste(map(text, function(number) word_index_df %>%
filter(idx == number) %>%
select(word) %>%
pull()),
collapse = " ")
}
decode_review(train_data[[7]])
# padding at the end
train_data <- pad_sequences(
train_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
test_data <- pad_sequences(
test_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
# pick 10000 values from original training data
x_val <- train_data[1:10000, ]
partial_x_train <- train_data[10001:nrow(train_data), ]
y_val <- train_labels[1:10000]
partial_y_train <- train_labels[10001:length(train_labels)]
newList <- list("x_val" = x_val,"partial_x_train" = partial_x_train,"y_val" = y_val,"partial_y_train" = partial_y_train,"test_data"=test_data,"test_labels"=test_labels)
#return(newList)
#}
train_data[[7]]
#Movie_data <- function(){
imdb <- dataset_imdb(num_words = 10000)
c(train_data, train_labels) %<-% imdb$train
c(test_data, test_labels) %<-% imdb$test
#num words discards rare words and keeps 10000 most used words
# maps words to integers
word_index <- dataset_imdb_word_index()
paste0("Training entries: ", length(train_data), ", labels: ", length(train_labels))
# integer form for review 1
#train_data[[1]]
# reviews should be same length - resolved later
#length(train_data[[1]])
#length(train_data[[2]])
word_index_df <- data.frame(
word = names(word_index),
idx = unlist(word_index, use.names = FALSE),
stringsAsFactors = FALSE
)
# The first indices are reserved
word_index_df <- word_index_df %>% mutate(idx = idx + 3)
word_index_df <- word_index_df %>%
add_row(word = "<PAD>", idx = 0)%>%
add_row(word = "<START>", idx = 1)%>%
add_row(word = "<UNK>", idx = 2)%>%
add_row(word = "<UNUSED>", idx = 3)
word_index_df <- word_index_df %>% arrange(idx)
decode_review <- function(text){
paste(map(text, function(number) word_index_df %>%
filter(idx == number) %>%
select(word) %>%
pull()),
collapse = " ")
}
train_data[[7]]
decode_review(train_data[[7]])
# padding at the end
train_data <- pad_sequences(
train_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
test_data <- pad_sequences(
test_data,
value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
padding = "post",
maxlen = 256
)
# pick 10000 values from original training data
x_val <- train_data[1:10000, ]
partial_x_train <- train_data[10001:nrow(train_data), ]
y_val <- train_labels[1:10000]
partial_y_train <- train_labels[10001:length(train_labels)]
newList <- list("x_val" = x_val,"partial_x_train" = partial_x_train,"y_val" = y_val,"partial_y_train" = partial_y_train,"test_data"=test_data,"test_labels"=test_labels)
#return(newList)
#}
library(keras)
library(tidyr)
library(ggplot2)
fashion_mnist <- dataset_fashion_mnist()
c(train_images, train_labels) %<-% fashion_mnist$train
c(test_images, test_labels) %<-% fashion_mnist$test
class_names = c('T-shirt/top',
'Trouser',
'Pullover',
'Dress',
'Coat',
'Sandal',
'Shirt',
'Sneaker',
'Bag',
'Ankle boot')
dim(train_images)
#train_labels[1:20]
dim(test_images)
dim(test_labels)
image_1 <- as.data.frame(train_images[1, , ])
colnames(image_1) <- seq_len(ncol(image_1))
image_1$y <- seq_len(nrow(image_1))
image_1 <- gather(image_1, "x", "value", -y)
image_1$x <- as.integer(image_1$x)
ggplot(image_1, aes(x = x, y = y, fill = value)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "black", na.value = NA) +
scale_y_reverse() +
theme_minimal() +
theme(panel.grid = element_blank())   +
theme(aspect.ratio = 1) +
xlab("") +
ylab("")
# rescale
train_images <- train_images / 255
test_images <- test_images / 255
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) {
img <- train_images[i, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste(class_names[train_labels[i] + 1]))
}
model <- keras_model_sequential() # transforms data into the right form
model %>%
layer_flatten(input_shape = c(28, 28)) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'sparse_categorical_crossentropy',
metrics = c('accuracy')
)
model %>% fit(train_images, train_labels, epochs = 5)
score <- model %>% evaluate(test_images, test_labels)
cat('Test loss:', score$loss, "\n")
cat('Test accuracy:', score$acc, "\n")
predictions <- model %>% predict(test_images)
predictions[1, ]
which.max(predictions[1, ])
class_pred <- model %>% predict_classes(test_images)
class_pred[1:20]
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) {
img <- test_images[i, , ]
img <- t(apply(img, 2, rev)) #matrix transpose, rotate
# subtract 1 as labels go from 0 to 9
predicted_label <- which.max(predictions[i, ]) - 1
true_label <- test_labels[i]
if (predicted_label == true_label) {
color <- '#008800'
} else {
color <- '#bb0000'
}
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n', # each image is 28x28 size, suppress x and y axis
main = paste0(class_names[predicted_label + 1], " (",
class_names[true_label + 1], ")"), # prediction (true label) quoted
col.main = color) # green if right, red if wrong
}
# Grab an image from the test dataset
# take care to keep the batch dimension, as this is expected by the model
img <- test_images[1, , , drop = FALSE]
dim(img)
predictions <- model %>% predict(img)
predictions
# subtract 1 as labels are 0-based
prediction <- predictions[1, ] - 1
which.max(prediction)
class_pred <- model %>% predict_classes(img)
class_pred
source('~/GitHub/NIAB_Rotation/Fruit_model/Run/params.R')
source('~/GitHub/NIAB_Rotation/Fruit_model/Data/Data_Functions.R') # not in parallel # contains functions, 'labeller', 'Data_in_final_form'
source('~/GitHub/NIAB_Rotation/Fruit_model/Data/Cluster_data_functions.R') # in parallel
source('~/GitHub/NIAB_Rotation/Fruit_model/Analysis/Functions.R') # contains functions 'image_tester', 'preds', 'multipreds', 'image_predictor'
source('~/GitHub/NIAB_Rotation/Fruit_model/Model/Fruit_model.R')  # needs params   # contains function 'create_fruit_model' # needs to be before data_producer, so library(keras) is before library(reticulate)?
source('~/GitHub/NIAB_Rotation/Fruit_model/Data/Data_Producer.R') # slow to run, saves train & test data & labels
library(keras)
library(dplyr)
library(ggplot2)
library(tidyr)
# data comes in from Data_Producer
Train_data_saved   <- readRDS(params$train_data_name)
Train_labels_saved <- readRDS(params$train_label_name)
Test_data_saved    <- readRDS(params$test_data_name)
Test_labels_saved  <- readRDS(params$test_label_name)
############################################
# fit model
model_name <- "fruit_model_new.h5" # "fruit_model.h5"
model_my_own <-create_fruit_model(Train_data_saved[,,,params$channel_no,drop = F],Train_labels_saved,Test_data_saved[,,,params$channel_no,drop = F],Test_labels_saved) # drop = F stops R collapsing array to 3 dimensions not 4
model_my_own %>% summary()
model_my_own %>% save_model_hdf5(model_name)
# below model has all fruits
#new_model <- load_model_hdf5("fruit_model.h5")
#new_model %>% summary()
results <- model_my_own %>% evaluate(Test_data_saved[,,,params$channel_no,drop=F], Test_labels_saved)
results
results <- model_my_own %>% predict_classes(Test_data_saved[,,,params$channel_no,drop = F]) # drop = F stops R collapsing array to 3 dimensions not 4
############################################
data <- image_tester(params$internet_path,"Braeburn")
res2 <- model_my_own %>% predict(data)
preds(res2)
multipreds(res2,params$number_probs)
results
results <- model_my_own %>% evaluate(Test_data_saved[,,,params$channel_no,drop=F], Test_labels_saved)
results
labels_to_be_tested <- as.numeric(filter(class_names_frame,Fruit %in% fruit_list)[,2]) ## In par
class_names
grid_resulting_table

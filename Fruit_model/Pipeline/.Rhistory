validation_data,
target_height = target_height,
target_width = target_width,
shuffle = FALSE,
batch_size = batch_size
)
model %>% fit_generator(
train_gen,
epochs = 20,
steps_per_epoch = nrow(train_data) / batch_size,
validation_data = valid_gen,
validation_steps = nrow(validation_data) / batch_size,
callbacks = list(
callback_model_checkpoint(
file.path("loc_only", "weights.{epoch:02d}-{val_loss:.2f}.hdf5")
),
callback_early_stopping(patience = 2)
)
)
plot_image_with_boxes <- function(file_name,
object_class,
box,
scaled = FALSE,
class_pred = NULL,
box_pred = NULL) {
img <- image_read(file.path(img_dir, file_name))
if(scaled) img <- image_resize(img, geometry = "224x224!")
img <- image_draw(img)
x_left <- box[1]
y_bottom <- box[2]
x_right <- box[3]
y_top <- box[4]
rect(
x_left,
y_bottom,
x_right,
y_top,
border = "cyan",
lwd = 2.5
)
text(
x_left,
y_top,
object_class,
offset = 1,
pos = 2,
cex = 1.5,
col = "cyan"
)
if (!is.null(box_pred))
rect(box_pred[1],
box_pred[2],
box_pred[3],
box_pred[4],
border = "yellow",
lwd = 2.5)
if (!is.null(class_pred))
text(
box_pred[1],
box_pred[2],
class_pred,
offset = 0,
pos = 4,
cex = 1.5,
col = "yellow")
dev.off()
img %>% image_write(paste0("preds_", file_name))
plot(img)
}
train_1_8 <- train_data[1:8, c("file_name",
"name",
"x_left_scaled",
"y_top_scaled",
"x_right_scaled",
"y_bottom_scaled")]
for (i in 1:8) {
preds <-
model %>% predict(
load_and_preprocess_image(train_1_8[i, "file_name"],
target_height, target_width),
batch_size = 1
)
plot_image_with_boxes(train_1_8$file_name[i],
train_1_8$name[i],
train_1_8[i, 3:6] %>% as.matrix(),
scaled = TRUE,
box_pred = preds)
}
install_keras(tensorflow = "gpu")
install_keras(tensorflow = "gpu")
library(keras)
install_keras(tensorflow = "gpu")
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
img_dir <- "C:/Users/Administrator/Documents/Nick/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages"
annot_file <- "C:/Users/Administrator/Documents/PASCAL_VOC/pascal_train2007.json"
annotations <- fromJSON(file = annot_file)
str(annotations, max.level = 1)
imageinfo <- annotations$images %>% {
tibble(
id = map_dbl(., "id"),
file_name = map_chr(., "file_name"),
image_height = map_dbl(., "height"),
image_width = map_dbl(., "width")
)
}
classes <- c(
"aeroplane",
"bicycle",
"bird",
"boat",
"bottle",
"bus",
"car",
"cat",
"chair",
"cow",
"diningtable",
"dog",
"horse",
"motorbike",
"person",
"pottedplant",
"sheep",
"sofa",
"train",
"tvmonitor"
)
boxinfo <- annotations$annotations %>% {
tibble(
image_id = map_dbl(., "image_id"),
category_id = map_dbl(., "category_id"),
bbox = map(., "bbox")
)
}
boxinfo <- boxinfo %>%
mutate(bbox = unlist(map(.$bbox, function(x) paste(x, collapse = " "))))
boxinfo <- boxinfo %>%
separate(bbox, into = c("x_left", "y_top", "bbox_width", "bbox_height"))
boxinfo <- boxinfo %>% mutate_all(as.numeric)
boxinfo <- boxinfo %>%
mutate(y_bottom = y_top + bbox_height - 1, x_right = x_left + bbox_width - 1)
catinfo <- annotations$categories %>%  {
tibble(id = map_dbl(., "id"), name = map_chr(., "name"))
}
imageinfo <- imageinfo %>%
inner_join(boxinfo, by = c("id" = "image_id")) %>%
inner_join(catinfo, by = c("category_id" = "id"))
target_height <- 224
target_width <- 224
imageinfo <- imageinfo %>% mutate(
x_left_scaled = (x_left / image_width * target_width) %>% round(),
x_right_scaled = (x_right / image_width * target_width) %>% round(),
y_top_scaled = (y_top / image_height * target_height) %>% round(),
y_bottom_scaled = (y_bottom / image_height * target_height) %>% round(),
bbox_width_scaled =  (bbox_width / image_width * target_width) %>% round(),
bbox_height_scaled = (bbox_height / image_height * target_height) %>% round()
)
img_data <- imageinfo[4,]
img <- image_read(file.path(img_dir, img_data$file_name))
img <- image_draw(img)
rect(
img_data$x_left,
img_data$y_bottom,
img_data$x_right,
img_data$y_top,
border = "white",
lwd = 2
)
text(
img_data$x_left,
img_data$y_top,
img_data$name,
offset = 1,
pos = 2,
cex = 1.5,
col = "white"
)
dev.off()
imageinfo <- imageinfo %>% mutate(area = bbox_width_scaled * bbox_height_scaled)
imageinfo_maxbb <- imageinfo %>%
group_by(id) %>%
filter(which.max(area) == row_number())
n_samples<-2000 # me
train_indices <- sample(1:n_samples, 0.01 * n_samples)
train_data <- imageinfo_maxbb[train_indices,]
validation_data <- imageinfo_maxbb[-train_indices,]
#### single object classification
feature_extractor <-
application_xception(
include_top = FALSE,
input_shape = c(224, 224, 3),
pooling = "avg"
)
feature_extractor %>% freeze_weights()
model <- keras_model_sequential() %>%
feature_extractor %>%
layer_batch_normalization() %>%
layer_dropout(rate = 0.25) %>%
layer_dense(units = 512, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 20, activation = "softmax")
model %>% compile(
optimizer = "adam",
loss = "sparse_categorical_crossentropy",
metrics = list("accuracy")
)
batch_size <- 10
load_and_preprocess_image <- function(image_name, target_height, target_width) {
img_array <- image_load(
file.path(img_dir, image_name),
target_size = c(target_height, target_width)
) %>%
image_to_array() %>%
xception_preprocess_input()
dim(img_array) <- c(1, dim(img_array))
img_array
}
classification_generator <-
function(data,
target_height,
target_width,
shuffle,
batch_size) {
i <- 1
function() {
if (shuffle) {
indices <- sample(1:nrow(data), size = batch_size)
} else {
if (i + batch_size >= nrow(data))
i <<- 1
indices <- c(i:min(i + batch_size - 1, nrow(data)))
i <<- i + length(indices)
}
x <-
array(0, dim = c(length(indices), target_height, target_width, 3))
y <- array(0, dim = c(length(indices), 1))
for (j in 1:length(indices)) {
x[j, , , ] <-
load_and_preprocess_image(data[[indices[j], "file_name"]],
target_height, target_width)
y[j, ] <-
data[[indices[j], "category_id"]] - 1
}
x <- x / 255
list(x, y)
}
}
train_gen <- classification_generator(
train_data,
target_height = target_height,
target_width = target_width,
shuffle = TRUE,
batch_size = batch_size
)
valid_gen <- classification_generator(
validation_data,
target_height = target_height,
target_width = target_width,
shuffle = FALSE,
batch_size = batch_size
)
model %>% fit_generator(
train_gen,
epochs = 20,
steps_per_epoch = nrow(train_data) / batch_size,
validation_data = valid_gen,
validation_steps = nrow(validation_data) / batch_size,
callbacks = list(
callback_model_checkpoint(
file.path("class_only", "weights.{epoch:02d}-{val_loss:.2f}.hdf5")
),
callback_early_stopping(patience = 2)
)
)
source('~/GitHub/NIAB_Rotation/Fruit_model/Run/params.R')
source('~/GitHub/NIAB_Rotation/Fruit_model/Data/Data_Functions.R') # not in parallel # contains functions, 'labeller', 'Data_in_final_form'
source('~/GitHub/NIAB_Rotation/Fruit_model/Data/Cluster_data_functions.R') # in parallel
source('~/GitHub/NIAB_Rotation/Fruit_model/Analysis/Functions.R') # contains functions 'image_tester', 'preds', 'multipreds', 'image_predictor'
source('~/GitHub/NIAB_Rotation/Fruit_model/Model/Fruit_model.R')  # needs params   # contains function 'create_fruit_model' # needs to be before data_producer, so library(keras) is before library(reticulate)?
source('~/GitHub/NIAB_Rotation/Fruit_model/Data/Data_Producer.R') # slow to run, saves train & test data & labels
library(keras)
library(dplyr)
library(ggplot2)
library(tidyr)
# data comes in from Data_Producer
Train_data_saved   <- readRDS(params$train_data_name)
Train_labels_saved <- readRDS(params$train_label_name)
Test_data_saved    <- readRDS(params$test_data_name)
Test_labels_saved  <- readRDS(params$test_label_name)
############################################
# fit model
model_name <- "fruit_model_new.h5" # "fruit_model.h5"
model_my_own <-create_fruit_model(Train_data_saved[,,,params$channel_no,drop = F],Train_labels_saved,Test_data_saved[,,,params$channel_no,drop = F],Test_labels_saved) # drop = F stops R collapsing array to 3 dimensions not 4
model_my_own %>% summary()
model_my_own %>% save_model_hdf5(model_name)
# below model has all fruits
#new_model <- load_model_hdf5("fruit_model.h5")
#new_model %>% summary()
results <- model_my_own %>% evaluate(Test_data_saved[,,,params$channel_no,drop=F], Test_labels_saved)
results
results <- model_my_own %>% predict_classes(Test_data_saved[,,,params$channel_no,drop = F]) # drop = F stops R collapsing array to 3 dimensions not 4
############################################
data <- image_tester(params$internet_path,"Braeburn")
res2 <- model_my_own %>% predict(data)
preds(res2)
multipreds(res2,params$number_probs)
params <- list('img_dir' = "C:/Users/Administrator/Documents/GitHub/test_images_to_use/all2",
'annot_file' = "C:/Users/Administrator/Documents/GitHub/test_images_to_use/jsonfold/online.json",
'folder_containing_scripts' = "C:/Users/Administrator/Documents/GitHub/NIAB_Rotation/Fruit_model/Pipeline",
'folder_to_save_model_in' = "C:/Users/Administrator/Documents/GitHub",
'folder_to_save_images_in' = "C:/Users/Administrator/Documents/GitHub/Pipeline_resulting_images",
'target_height' = 224,
'target_width' = 224,
'batch_size' = 1, #10 #1 # low is faster but less accurate?   ### 4
'proportion_of_samples' = 0.1, # 0.2, but was classifying everything the same. ###0.3
'threshold' = 0.4,
'class_background' = length(class_list), # should it be length(class_list), or length(class_list) + 1?
'cl_output' = length(class_list), # 20
'epochs' = 20,
'weight_file_path' = "C:/Users/Administrator/Documents/GitHub/Weights",
'label_names' = class_list,
'layer_units' = 16, #256, # 30
'patience' = 2, # was 8, but that's quite slow
'save' = 1, #save model?
'model_name' = "disease_image_classifier.h5"
)
class_list <- c("YR","MSD","BS") # from json
params <- list('img_dir' = "C:/Users/Administrator/Documents/GitHub/test_images_to_use/all2",
'annot_file' = "C:/Users/Administrator/Documents/GitHub/test_images_to_use/jsonfold/online.json",
'folder_containing_scripts' = "C:/Users/Administrator/Documents/GitHub/NIAB_Rotation/Fruit_model/Pipeline",
'folder_to_save_model_in' = "C:/Users/Administrator/Documents/GitHub",
'folder_to_save_images_in' = "C:/Users/Administrator/Documents/GitHub/Pipeline_resulting_images",
'target_height' = 224,
'target_width' = 224,
'batch_size' = 1, #10 #1 # low is faster but less accurate?   ### 4
'proportion_of_samples' = 0.1, # 0.2, but was classifying everything the same. ###0.3
'threshold' = 0.4,
'class_background' = length(class_list), # should it be length(class_list), or length(class_list) + 1?
'cl_output' = length(class_list), # 20
'epochs' = 20,
'weight_file_path' = "C:/Users/Administrator/Documents/GitHub/Weights",
'label_names' = class_list,
'layer_units' = 16, #256, # 30
'patience' = 2, # was 8, but that's quite slow
'save' = 1, #save model?
'model_name' = "disease_image_classifier.h5"
)
source('C:/Users/Administrator/Documents/GitHub/NIAB_Rotation/Fruit_model/Pipeline/parameters.R')
setwd(params$folder_containing_scripts)
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(XML)
library(xml2)
library(jsonlite)
library(tensorflow)
# with set.seed(142) we should get in CNN_data_generator_and_model_functions
# train_indicies = [77 60 81 47 67 71 23 54 34 78 66 82 57 41 38 40 11 37 84 12 62 61  4 29 56]
# suppress_warnings <- 1
# oldw <- getOption("warn")
# if(suppress_warnings==1) options(warn = -1) # suppress warnings as they slow it down
run_model_trainer<-0 # train model?
params$load <- 1 # if CNN_model is already in the environment, can change to params$load <- 0 to save computational time
run_xml_to_json<-0
run_analyse_CNN_output<-1
run_analyse_SVM_output<-0
if(run_xml_to_json==1){
system("python xmltojson.py")
}
source('Image_classifier_functions.R')
source('C:/Users/Administrator/Documents/GitHub/NIAB_Rotation/Fruit_model/Pipeline/parameters.R')
setwd(params$folder_containing_scripts)
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(XML)
library(xml2)
library(jsonlite)
library(tensorflow)
# with set.seed(142) we should get in CNN_data_generator_and_model_functions
# train_indicies = [77 60 81 47 67 71 23 54 34 78 66 82 57 41 38 40 11 37 84 12 62 61  4 29 56]
# suppress_warnings <- 1
# oldw <- getOption("warn")
# if(suppress_warnings==1) options(warn = -1) # suppress warnings as they slow it down
run_model_trainer<-0 # train model?
params$load <- 1 # if CNN_model is already in the environment, can change to params$load <- 0 to save computational time
run_xml_to_json<-0
run_analyse_CNN_output<-1
run_analyse_SVM_output<-0
if(run_xml_to_json==1){
system("python xmltojson.py")
}
source('Image_classifier_functions.R')
source('CNN_data_generator_and_model_functions.R')
if(run_model_trainer==1){
source('CNN_model_trainer.R') # sources 'Image_classifier_functions.R', 'parameters.R', feeds into
}
if(run_analyse_CNN_output==1){
source('CNN_output_analysis.R')
val_analysis$class_predictions
val_analysis$corners
}
source('Disease_fake_data.R') # feeds into
source('SVM.R') # feeds into
if(run_analyse_SVM_output==1){
source('SVM_output_analysis.R')
head(attr(predz,"probabilities")) # predicted class
}
#if(suppress_warnings==1) options(warn = oldw) # allow warnings to happen again
source('SVM_output_analysis.R')
system("python xmltojson.py")
system("python xmltojson.py")
system("python xmltojson.py")
system("python xmltojson.py")
system("python xmltojson.py")
system("python xmltojson.py")
system("python xmltojson.py")
system("python xmltojson.py")
system("python xmltojson.py")
system("python xmltojson.py")
system("python xmltojson.py")
system("python xmltojson.py")
source('C:/Users/Administrator/Documents/GitHub/NIAB_Rotation/Fruit_model/Pipeline/parameters.R')
setwd(params$folder_containing_scripts)
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(XML)
library(xml2)
library(jsonlite)
library(tensorflow)
# with set.seed(142) we should get in CNN_data_generator_and_model_functions
# train_indicies = [77 60 81 47 67 71 23 54 34 78 66 82 57 41 38 40 11 37 84 12 62 61  4 29 56]
# suppress_warnings <- 1
# oldw <- getOption("warn")
# if(suppress_warnings==1) options(warn = -1) # suppress warnings as they slow it down
run_model_trainer<-0 # train model?
params$load <- 1 # if CNN_model is already in the environment, can change to params$load <- 0 to save computational time
run_xml_to_json<-0
run_analyse_CNN_output<-1
run_analyse_SVM_output<-0
if(run_xml_to_json==1){
system("python xmltojson.py")
}
source('Image_classifier_functions.R')
source('CNN_data_generator_and_model_functions.R')
if(run_model_trainer==1){
source('CNN_model_trainer.R') # sources 'Image_classifier_functions.R', 'parameters.R', feeds into
}
if(run_analyse_CNN_output==1){
source('CNN_output_analysis.R')
val_analysis$class_predictions
val_analysis$corners
}
source('Disease_fake_data.R') # feeds into
source('SVM.R') # feeds into
source('C:/Users/Administrator/Documents/GitHub/NIAB_Rotation/Fruit_model/Pipeline/parameters.R')
setwd(params$folder_containing_scripts)
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(XML)
library(xml2)
library(jsonlite)
library(tensorflow)
# with set.seed(142) we should get in CNN_data_generator_and_model_functions
# train_indicies = [77 60 81 47 67 71 23 54 34 78 66 82 57 41 38 40 11 37 84 12 62 61  4 29 56]
# suppress_warnings <- 1
# oldw <- getOption("warn")
# if(suppress_warnings==1) options(warn = -1) # suppress warnings as they slow it down
run_model_trainer<-0 # train model?
params$load <- 1 # if CNN_model is already in the environment, can change to params$load <- 0 to save computational time
run_xml_to_json<-0
run_analyse_CNN_output<-1
run_analyse_SVM_output<-0
if(run_xml_to_json==1){
system("python xmltojson.py")
}
source('Image_classifier_functions.R')
source('CNN_data_generator_and_model_functions.R')
